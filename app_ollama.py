import os
import json
import time
import pandas as pd
import gradio as gr
import numpy as np
import requests
from datetime import datetime

from langchain.vectorstores import FAISS
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.llms import OpenAI
from langchain.chains import RetrievalQA

# ---------------------------------------
# 1Ô∏è‚É£ –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏ –º–æ–¥–µ–ª–∏
# ---------------------------------------
# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è Ollama
OLLAMA_HOST = os.getenv("OLLAMA_HOST", "http://localhost:11434")
MODEL_NAME = os.getenv("MODEL_NAME", "llama3.2:8b")
USE_OLLAMA = os.getenv("USE_OLLAMA", "true").lower() == "true"

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è OpenAI (–µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è)
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º API –∫–ª—é—á –¥–ª—è embeddings (–≤—Å–µ–≥–¥–∞ –Ω—É–∂–µ–Ω –¥–ª—è FAISS)
if OPENAI_API_KEY:
    os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY
    print("üü¢ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è OpenAI –¥–ª—è embeddings")
else:
    print("‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: OPENAI_API_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –¥–ª—è embeddings!")

if USE_OLLAMA:
    print("ü¶ô –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ª–æ–∫–∞–ª—å–Ω–∞—è Ollama")
else:
    print("üü¢ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è OpenAI API")

DATA_DIR = "data"
FAQ_PATH = os.path.join(DATA_DIR, "faq.csv")
LOG_PATH = os.path.join(DATA_DIR, "logs", "chat_log.json")

# –°–æ–∑–¥–∞—ë–º –ø–∞–ø–∫—É –ª–æ–≥–æ–≤, –µ—Å–ª–∏ –Ω–µ—Ç
os.makedirs(os.path.join(DATA_DIR, "logs"), exist_ok=True)

# ---------------------------------------
# 2Ô∏è‚É£ –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å RAG –∏ –±–∞–∑–æ–π –∑–Ω–∞–Ω–∏–π
# ---------------------------------------

def load_faq():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç FAQ –∏–∑ CSV —Ñ–∞–π–ª–∞"""
    if not os.path.exists(FAQ_PATH):
        df = pd.DataFrame(columns=["question", "answer"])
        df.to_csv(FAQ_PATH, index=False)
    return pd.read_csv(FAQ_PATH)

def save_faq(df):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç FAQ –≤ CSV —Ñ–∞–π–ª"""
    df.to_csv(FAQ_PATH, index=False)

def build_vectorstore(df):
    """–°–æ–∑–¥–∞—ë—Ç FAISS-–≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É –∏–∑ CSV FAQ"""
    if df.empty:
        # –°–æ–∑–¥–∞—ë–º –ø—É—Å—Ç—É—é –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É –µ—Å–ª–∏ –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö
        embeddings = OpenAIEmbeddings(openai_api_key=os.environ["OPENAI_API_KEY"])
        store = FAISS.from_texts(["–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö"], embeddings)
        store.save_local(os.path.join(DATA_DIR, "faiss_index"))
        return store
    
    embeddings = OpenAIEmbeddings(openai_api_key=os.environ["OPENAI_API_KEY"])
    texts = df["question"].tolist()
    metadatas = [{"answer": a} for a in df["answer"]]
    store = FAISS.from_texts(texts, embeddings, metadatas=metadatas)
    store.save_local(os.path.join(DATA_DIR, "faiss_index"))
    return store

def load_vectorstore():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É –∏–ª–∏ —Å–æ–∑–¥–∞—ë—Ç –Ω–æ–≤—É—é"""
    if not os.path.exists(os.path.join(DATA_DIR, "faiss_index")):
        df = load_faq()
        return build_vectorstore(df)
    embeddings = OpenAIEmbeddings(openai_api_key=os.environ["OPENAI_API_KEY"])
    return FAISS.load_local(os.path.join(DATA_DIR, "faiss_index"), embeddings, allow_dangerous_deserialization=True)

# ---------------------------------------
# 3Ô∏è‚É£ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ (RAG + LLM)
# ---------------------------------------

def call_ollama_api(question, context=""):
    """–í—ã–∑—ã–≤–∞–µ—Ç –ª–æ–∫–∞–ª—å–Ω—É—é Ollama API"""
    try:
        url = f"{OLLAMA_HOST}/api/generate"
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
        if context:
            prompt = f"""–¢—ã –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –æ–ø–µ—Ä–∞—Ç–æ—Ä —Å–ª—É–∂–±—ã –ø–æ–¥–¥–µ—Ä–∂–∫–∏. –ù–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –æ—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –∫–ª–∏–µ–Ω—Ç–∞.

–ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}

–í–æ–ø—Ä–æ—Å –∫–ª–∏–µ–Ω—Ç–∞: {question}

–û—Ç–≤–µ—Ç—å –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ –∏ –≤–µ–∂–ª–∏–≤–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. –ï—Å–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–µ—Ç –Ω—É–∂–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º:"""
        else:
            prompt = f"""–¢—ã –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –æ–ø–µ—Ä–∞—Ç–æ—Ä —Å–ª—É–∂–±—ã –ø–æ–¥–¥–µ—Ä–∂–∫–∏. –û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –∫–ª–∏–µ–Ω—Ç–∞: {question}

–û—Ç–≤–µ—Ç—å –≤–µ–∂–ª–∏–≤–æ –∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ:"""
        
        data = {
            "model": MODEL_NAME,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": 0.7,
                "top_p": 0.9,
                "max_tokens": 500
            }
        }
        
        response = requests.post(url, json=data, timeout=60)
        response.raise_for_status()
        
        result = response.json()
        return result["response"]
        
    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ Ollama: {str(e)}"

def get_rag_response(question, chat_history):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –∏—Å–ø–æ–ª—å–∑—É—è RAG"""
    try:
        vectorstore = load_vectorstore()
        retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

        # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        docs = retriever.get_relevant_documents(question)
        context = "\n".join([doc.page_content for doc in docs])
        
        # –ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º Ollama
        if USE_OLLAMA:
            answer = call_ollama_api(question, context)
        else:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º LangChain —Å OpenAI
            qa_chain = RetrievalQA.from_chain_type(
                llm=OpenAI(model="gpt-3.5-turbo", temperature=0.3),
                retriever=retriever,
                return_source_documents=True
            )
            result = qa_chain({"query": question})
            answer = result["result"]

        # –õ–æ–≥–∏—Ä—É–µ–º
        log_interaction(question, answer)

        return answer
    except Exception as e:
        error_msg = f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {str(e)}"
        log_interaction(question, error_msg)
        return error_msg

# ---------------------------------------
# 4Ô∏è‚É£ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —á–∞—Ç–∞ –∏ –∏—Å—Ç–æ—Ä–∏–∏
# ---------------------------------------

def log_interaction(question, answer):
    """–õ–æ–≥–∏—Ä—É–µ—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å –±–æ—Ç–æ–º"""
    log = {
        "timestamp": datetime.now().isoformat(),
        "question": question,
        "answer": answer
    }
    logs = []
    if os.path.exists(LOG_PATH):
        with open(LOG_PATH, "r", encoding="utf-8") as f:
            try:
                logs = json.load(f)
            except:
                logs = []
    logs.append(log)
    with open(LOG_PATH, "w", encoding="utf-8") as f:
        json.dump(logs, f, ensure_ascii=False, indent=2)

def load_logs():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ª–æ–≥–∏ —á–∞—Ç–∞"""
    if not os.path.exists(LOG_PATH):
        return []
    with open(LOG_PATH, "r", encoding="utf-8") as f:
        try:
            return json.load(f)
        except:
            return []

# ---------------------------------------
# 5Ô∏è‚É£ –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å FAQ –∏ –∞–¥–º–∏–Ω-—Ñ—É–Ω–∫—Ü–∏–∏
# ---------------------------------------

def add_faq(question, answer):
    """–î–æ–±–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π FAQ"""
    if not question.strip() or not answer.strip():
        return "–í–æ–ø—Ä–æ—Å –∏ –æ—Ç–≤–µ—Ç –Ω–µ –º–æ–≥—É—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º–∏!", load_faq()
    
    df = load_faq()
    new_row = {"question": question, "answer": answer}
    df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)
    save_faq(df)
    build_vectorstore(df)
    return "–î–æ–±–∞–≤–ª–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!", df

def delete_faq(index):
    """–£–¥–∞–ª—è–µ—Ç FAQ –ø–æ –∏–Ω–¥–µ–∫—Å—É"""
    df = load_faq()
    if 0 <= index < len(df):
        df = df.drop(index).reset_index(drop=True)
        save_faq(df)
        build_vectorstore(df)
        return "–£–¥–∞–ª–µ–Ω–æ!", df
    return "–ù–µ–≤–µ—Ä–Ω—ã–π –∏–Ω–¥–µ–∫—Å.", df

def refresh_faq():
    """–û–±–Ω–æ–≤–ª—è–µ—Ç —Å–ø–∏—Å–æ–∫ FAQ"""
    return load_faq()

# ---------------------------------------
# 6Ô∏è‚É£ –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å Gradio
# ---------------------------------------

def chat_fn(message, history):
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –≤–∫–ª–∞–¥–∫–∏ '–ß–∞—Ç'"""
    if not message.strip():
        return history, ""
    
    answer = get_rag_response(message, history)
    history.append((message, answer))
    return history, ""

def render_logs():
    """–†–µ–Ω–¥–µ—Ä–∏—Ç –ª–æ–≥–∏ –≤ HTML —Ç–∞–±–ª–∏—Ü—É"""
    logs = load_logs()
    if not logs:
        return "–õ–æ–≥–æ–≤ –ø–æ–∫–∞ –Ω–µ—Ç."
    
    table = """
    <style>
    table { border-collapse: collapse; width: 100%; }
    th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
    th { background-color: #f2f2f2; }
    tr:nth-child(even) { background-color: #f9f9f9; }
    </style>
    <table>
    <tr><th>–î–∞—Ç–∞</th><th>–í–æ–ø—Ä–æ—Å</th><th>–û—Ç–≤–µ—Ç</th></tr>
    """
    
    for log in reversed(logs[-30:]):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –∑–∞–ø–∏—Å–µ–π
        timestamp = log['timestamp'][:19].replace('T', ' ')
        question = log['question'][:100] + "..." if len(log['question']) > 100 else log['question']
        answer = log['answer'][:200] + "..." if len(log['answer']) > 200 else log['answer']
        table += f"<tr><td>{timestamp}</td><td>{question}</td><td>{answer}</td></tr>"
    
    table += "</table>"
    return table

def clear_logs():
    """–û—á–∏—â–∞–µ—Ç –≤—Å–µ –ª–æ–≥–∏"""
    if os.path.exists(LOG_PATH):
        os.remove(LOG_PATH)
    return "–õ–æ–≥–∏ –æ—á–∏—â–µ–Ω—ã!"

# –°–æ–∑–¥–∞—ë–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
with gr.Blocks(title="RAG Support Bot - Ollama", theme=gr.themes.Soft()) as demo:
    gr.Markdown("# RAG Support Bot ‚Äî –ø–∞–Ω–µ–ª—å –ø–æ–¥–¥–µ—Ä–∂–∫–∏ —Å Ollama")
    
    with gr.Tab("–ß–∞—Ç"):
        gr.Markdown("–ó–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å –±–æ—Ç—É –ø–æ–¥–¥–µ—Ä–∂–∫–∏. –û–Ω –Ω–∞–π–¥–µ—Ç –æ—Ç–≤–µ—Ç –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π.")
        chatbot = gr.Chatbot(label="–î–∏–∞–ª–æ–≥ —Å –±–æ—Ç–æ–º", height=400)
        msg = gr.Textbox(label="–í–≤–µ–¥–∏—Ç–µ –≤–æ–ø—Ä–æ—Å", placeholder="–ö–∞–∫ —Å–±—Ä–æ—Å–∏—Ç—å –ø–∞—Ä–æ–ª—å?")
        send = gr.Button("–û—Ç–ø—Ä–∞–≤–∏—Ç—å", variant="primary")
        send.click(chat_fn, inputs=[msg, chatbot], outputs=[chatbot, msg])
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ Enter
        msg.submit(chat_fn, inputs=[msg, chatbot], outputs=[chatbot, msg])

    with gr.Tab("FAQ –±–∞–∑–∞"):
        gr.Markdown("–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –±–∞–∑–æ–π –∑–Ω–∞–Ω–∏–π. –î–æ–±–∞–≤–ª—è–π—Ç–µ –∏ —É–¥–∞–ª—è–π—Ç–µ –≤–æ–ø—Ä–æ—Å—ã –∏–∑ FAQ.")
        
        with gr.Row():
            with gr.Column():
                q_inp = gr.Textbox(label="–í–æ–ø—Ä–æ—Å", placeholder="–ö–∞–∫ –∏–∑–º–µ–Ω–∏—Ç—å email?")
                a_inp = gr.Textbox(label="–û—Ç–≤–µ—Ç", placeholder="–ü–µ—Ä–µ–π–¥–∏—Ç–µ –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—Ä–æ—Ñ–∏–ª—è...", lines=3)
                add_btn = gr.Button("–î–æ–±–∞–≤–∏—Ç—å FAQ", variant="primary")
            
            with gr.Column():
                delete_index = gr.Number(label="–ò–Ω–¥–µ–∫—Å –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è", value=0)
                delete_btn = gr.Button("–£–¥–∞–ª–∏—Ç—å FAQ", variant="stop")
                refresh_btn = gr.Button("–û–±–Ω–æ–≤–∏—Ç—å —Å–ø–∏—Å–æ–∫")
        
        output = gr.Textbox(label="–°—Ç–∞—Ç—É—Å –æ–ø–µ—Ä–∞—Ü–∏–∏")
        faq_table = gr.Dataframe(load_faq(), label="–¢–µ–∫—É—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π")

        add_btn.click(add_faq, inputs=[q_inp, a_inp], outputs=[output, faq_table])
        delete_btn.click(delete_faq, inputs=[delete_index], outputs=[output, faq_table])
        refresh_btn.click(refresh_faq, outputs=faq_table)

    with gr.Tab("–ù–∞—Å—Ç—Ä–æ–π–∫–∏"):
        gr.Markdown("### –¢–µ–∫—É—â–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–∏—Å—Ç–µ–º—ã")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞–∫–æ–π API –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
        if USE_OLLAMA:
            gr.Markdown(f"**ü¶ô API –ü—Ä–æ–≤–∞–π–¥–µ—Ä:** Ollama (–ª–æ–∫–∞–ª—å–Ω–æ)")
            gr.Markdown(f"**–ú–æ–¥–µ–ª—å:** {MODEL_NAME}")
            gr.Markdown(f"**Ollama Host:** {OLLAMA_HOST}")
            gr.Markdown(f"**–°—Ç–∞—Ç—É—Å:** {'‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω' if check_ollama_connection() else '‚ùå –ù–µ–¥–æ—Å—Ç—É–ø–µ–Ω'}")
        else:
            gr.Markdown(f"**üü¢ API –ü—Ä–æ–≤–∞–π–¥–µ—Ä:** OpenAI")
            gr.Markdown(f"**OpenAI API Key:** {'‚úÖ –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω' if OPENAI_API_KEY else '‚ùå –ù–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω'}")
        
        gr.Markdown(f"**–ü—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º:** {DATA_DIR}")
        gr.Markdown(f"**–ü—É—Ç—å –∫ –ª–æ–≥–∞–º:** {LOG_PATH}")
        
        gr.Markdown("### –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ")
        gr.Markdown("""
        **–î–ª—è Ollama (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è):**
        1. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ Ollama: https://ollama.com/download
        2. –ó–∞–ø—É—Å—Ç–∏—Ç–µ: `ollama serve`
        3. –°–∫–∞—á–∞–π—Ç–µ –º–æ–¥–µ–ª—å: `ollama pull llama3.2:8b`
        4. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ `USE_OLLAMA=true`
        
        **–î–ª—è OpenAI API:**
        1. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è `OPENAI_API_KEY`
        2. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ `USE_OLLAMA=false`
        """)

    with gr.Tab("–õ–æ–≥–∏"):
        gr.Markdown("–ò—Å—Ç–æ—Ä–∏—è –≤—Å–µ—Ö –æ–±—Ä–∞—â–µ–Ω–∏–π –∫ –±–æ—Ç—É")
        logs_html = gr.HTML(render_logs())
        clear_logs_btn = gr.Button("–û—á–∏—Å—Ç–∏—Ç—å –ª–æ–≥–∏", variant="stop")
        refresh_logs_btn = gr.Button("–û–±–Ω–æ–≤–∏—Ç—å –ª–æ–≥–∏")
        
        def update_logs():
            return render_logs()
        
        clear_logs_btn.click(clear_logs, outputs=output)
        refresh_logs_btn.click(update_logs, outputs=logs_html)

def check_ollama_connection():
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Ollama"""
    try:
        response = requests.get(f"{OLLAMA_HOST}/api/tags", timeout=5)
        return response.status_code == 200
    except:
        return False

if __name__ == "__main__":
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Ollama
    if USE_OLLAMA:
        if check_ollama_connection():
            print("‚úÖ Ollama –ø–æ–¥–∫–ª—é—á–µ–Ω –∏ —Ä–∞–±–æ—Ç–∞–µ—Ç")
        else:
            print("‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω!")
            print("–ó–∞–ø—É—Å—Ç–∏—Ç–µ Ollama: ollama serve")
    else:
        if not OPENAI_API_KEY:
            print("‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: OPENAI_API_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω!")
        else:
            print("‚úÖ OpenAI API –∫–ª—é—á –Ω–∞–π–¥–µ–Ω")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
    port = int(os.getenv("PORT", 7860))
    print(f"üöÄ –ó–∞–ø—É—Å–∫ RAG Support Bot –Ω–∞ –ø–æ—Ä—Ç—É {port}")
    print(f"üåê –û—Ç–∫—Ä–æ–π—Ç–µ –±—Ä–∞—É–∑–µ—Ä: http://localhost:{port}")
    demo.launch(server_name="0.0.0.0", server_port=port)
