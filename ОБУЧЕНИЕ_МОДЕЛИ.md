# üéì –ü–æ–ª–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ –æ–±—É—á–µ–Ω–∏—é –º–æ–¥–µ–ª–∏

## üöÄ –ë—ã—Å—Ç—Ä—ã–π –∑–∞–ø—É—Å–∫

### 1. –ó–∞–ø—É—Å—Ç–∏—Ç–µ Ollama —Å–µ—Ä–≤–µ—Ä:
```bash
ollama serve
```

### 2. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ:
```bash
python app_simple_ollama.py
```

### 3. –û—Ç–∫—Ä–æ–π—Ç–µ –±—Ä–∞—É–∑–µ—Ä:
```
http://localhost:7860
```

## üìö –ö–∞–∫ –¥–æ–±–∞–≤–ª—è—Ç—å –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ –ø—Ä–æ–º–ø—Ç—ã

### üîß –ú–µ—Ç–æ–¥ 1: –ß–µ—Ä–µ–∑ –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å

**1. –î–æ–±–∞–≤–ª–µ–Ω–∏–µ FAQ:**
- –û—Ç–∫—Ä–æ–π—Ç–µ –≤–∫–ª–∞–¥–∫—É "FAQ –±–∞–∑–∞"
- –í–≤–µ–¥–∏—Ç–µ –≤–æ–ø—Ä–æ—Å –≤ –ø–æ–ª–µ "–í–æ–ø—Ä–æ—Å"
- –í–≤–µ–¥–∏—Ç–µ –æ—Ç–≤–µ—Ç –≤ –ø–æ–ª–µ "–û—Ç–≤–µ—Ç"
- –ù–∞–∂–º–∏—Ç–µ "–î–æ–±–∞–≤–∏—Ç—å FAQ"

**2. –ü—Ä–æ—Å–º–æ—Ç—Ä –ª–æ–≥–æ–≤:**
- –ü–µ—Ä–µ–π–¥–∏—Ç–µ –Ω–∞ –≤–∫–ª–∞–¥–∫—É "–õ–æ–≥–∏"
- –ü–æ—Å–º–æ—Ç—Ä–∏—Ç–µ –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–æ–≤
- –ê–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–æ–≤

### üîß –ú–µ—Ç–æ–¥ 2: –ü—Ä—è–º–æ–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤

**1. –†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ FAQ:**
–û—Ç–∫—Ä–æ–π—Ç–µ —Ñ–∞–π–ª `data/faq.json`:
```json
[
  {
    "question": "–ö–∞–∫ —Å–±—Ä–æ—Å–∏—Ç—å –ø–∞—Ä–æ–ª—å?",
    "answer": "–ü–µ—Ä–µ–π–¥–∏—Ç–µ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É –≤—Ö–æ–¥–∞ –∏ –Ω–∞–∂–º–∏—Ç–µ '–ó–∞–±—ã–ª–∏ –ø–∞—Ä–æ–ª—å?'..."
  },
  {
    "question": "–ù–æ–≤—ã–π –≤–æ–ø—Ä–æ—Å",
    "answer": "–ù–æ–≤—ã–π –æ—Ç–≤–µ—Ç"
  }
]
```

**2. –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è:**
–û—Ç–∫—Ä–æ–π—Ç–µ —Ñ–∞–π–ª `data/training/support_conversations.json`:
```json
{
  "conversations": [
    {
      "question": "–ö–∞–∫ –∏–∑–º–µ–Ω–∏—Ç—å –Ω–∞—Å—Ç—Ä–æ–π–∫–∏?",
      "answer": "–ü–µ—Ä–µ–π–¥–∏—Ç–µ –≤ –º–µ–Ω—é '–ù–∞—Å—Ç—Ä–æ–π–∫–∏' –≤ –ø—Ä–∞–≤–æ–º –≤–µ—Ä—Ö–Ω–µ–º —É–≥–ª—É...",
      "category": "settings"
    }
  ]
}
```

## üéØ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏

### üìù –®–∞–≥ 1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö

**–°–æ–∑–¥–∞–π—Ç–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –¥–∏–∞–ª–æ–≥–æ–≤:**

```json
{
  "conversations": [
    {
      "question": "–ù–µ –º–æ–≥—É –≤–æ–π—Ç–∏ –≤ –∞–∫–∫–∞—É–Ω—Ç",
      "answer": "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –≤–≤–æ–¥–∞ email –∏ –ø–∞—Ä–æ–ª—è. –ï—Å–ª–∏ –ø—Ä–æ–±–ª–µ–º–∞ –æ—Å—Ç–∞–µ—Ç—Å—è, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–±—Ä–æ—Å–∏—Ç—å –ø–∞—Ä–æ–ª—å.",
      "category": "login"
    },
    {
      "question": "–ó–∞–±—ã–ª –ø–∞—Ä–æ–ª—å",
      "answer": "–ù–∞–∂–º–∏—Ç–µ '–ó–∞–±—ã–ª–∏ –ø–∞—Ä–æ–ª—å?' –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –≤—Ö–æ–¥–∞, –≤–≤–µ–¥–∏—Ç–µ email –∏ —Å–ª–µ–¥—É–π—Ç–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –≤ –ø–∏—Å—å–º–µ.",
      "category": "password"
    }
  ]
}
```

### üîÑ –®–∞–≥ 2: –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏

**1. –°–æ–∑–¥–∞–π—Ç–µ –Ω–æ–≤—ã–π Modelfile:**
```dockerfile
FROM llama3.2:3b

SYSTEM """–¢—ã –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –æ–ø–µ—Ä–∞—Ç–æ—Ä —Å–ª—É–∂–±—ã –ø–æ–¥–¥–µ—Ä–∂–∫–∏. 
–¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –ø–æ–º–æ–≥–∞—Ç—å –∫–ª–∏–µ–Ω—Ç–∞–º —Ä–µ—à–∞—Ç—å –∏—Ö –ø—Ä–æ–±–ª–µ–º—ã.

–ò–ù–°–¢–†–£–ö–¶–ò–ò:
- –û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ
- –ë—É–¥—å –≤–µ–∂–ª–∏–≤—ã–º –∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–º
- –ò—Å–ø–æ–ª—å–∑—É–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
- –ï—Å–ª–∏ –Ω–µ –∑–Ω–∞–µ—à—å –æ—Ç–≤–µ—Ç–∞, –ø—Ä–µ–¥–ª–æ–∂–∏ —Å–≤—è–∑–∞—Ç—å—Å—è —Å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–º
- –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–π –æ—Ç–≤–µ—Ç—ã —á–µ—Ç–∫–æ –∏ –ø–æ–Ω—è—Ç–Ω–æ

–ü–†–ò–ú–ï–†–´ –û–¢–í–ï–¢–û–í:
–í: –ö–∞–∫ —Å–±—Ä–æ—Å–∏—Ç—å –ø–∞—Ä–æ–ª—å?
–û: –î–ª—è —Å–±—Ä–æ—Å–∞ –ø–∞—Ä–æ–ª—è –ø–µ—Ä–µ–π–¥–∏—Ç–µ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É –≤—Ö–æ–¥–∞ –∏ –Ω–∞–∂–º–∏—Ç–µ '–ó–∞–±—ã–ª–∏ –ø–∞—Ä–æ–ª—å?'. –í–≤–µ–¥–∏—Ç–µ –≤–∞—à email –∏ —Å–ª–µ–¥—É–π—Ç–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –≤ –ø–∏—Å—å–º–µ.

–í: –ù–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –æ–ø–ª–∞—Ç–∞
–û: –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–∞–Ω–Ω—ã–µ –∫–∞—Ä—Ç—ã –∏ –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥–æ–π —Å–ø–æ—Å–æ–± –æ–ø–ª–∞—Ç—ã. –ï—Å–ª–∏ –ø—Ä–æ–±–ª–µ–º–∞ –æ—Å—Ç–∞–µ—Ç—Å—è, –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –≤ –±–∞–Ω–∫."""

PARAMETER temperature 0.7
PARAMETER top_p 0.9
```

**2. –°–æ–∑–¥–∞–π—Ç–µ –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å:**
```bash
ollama create support-bot-v2 -f Modelfile
```

**3. –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π—Ç–µ –º–æ–¥–µ–ª—å:**
```bash
ollama run support-bot-v2 "–ö–∞–∫ —Å–±—Ä–æ—Å–∏—Ç—å –ø–∞—Ä–æ–ª—å?"
```

### üéØ –®–∞–≥ 3: –î–æ–æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é LoRA (–ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π)

**1. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏:**
```bash
pip install transformers datasets accelerate peft torch
```

**2. –°–æ–∑–¥–∞–π—Ç–µ —Å–∫—Ä–∏–ø—Ç –¥–æ–æ–±—É—á–µ–Ω–∏—è `train_model.py`:**
```python
import json
import torch
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    TrainingArguments, 
    Trainer
)
from peft import LoraConfig, get_peft_model, TaskType
from datasets import Dataset

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
model_name = "meta-llama/Llama-3.2-3B"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LoRA
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
    lora_dropout=0.1,
    bias="none",
    task_type=TaskType.CAUSAL_LM
)

model = get_peft_model(model, lora_config)

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
def load_training_data():
    with open("data/training/support_conversations.json", "r", encoding="utf-8") as f:
        data = json.load(f)
    
    conversations = []
    for conv in data["conversations"]:
        prompt = f"–í–æ–ø—Ä–æ—Å: {conv['question']}\n–û—Ç–≤–µ—Ç: {conv['answer']}"
        conversations.append({"text": prompt})
    
    return conversations

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
data = load_training_data()
dataset = Dataset.from_list(data)

def tokenize_function(examples):
    return tokenizer(
        examples["text"],
        truncation=True,
        padding=True,
        max_length=512
    )

tokenized_dataset = dataset.map(tokenize_function, batched=True)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—É—á–µ–Ω–∏—è
training_args = TrainingArguments(
    output_dir="./data/models/support-bot-lora",
    num_train_epochs=3,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    warmup_steps=100,
    learning_rate=2e-4,
    fp16=True,
    logging_steps=10,
    save_steps=100,
    evaluation_strategy="no",
    save_total_limit=2,
)

# –°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–µ–Ω–µ—Ä–∞
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    tokenizer=tokenizer,
)

# –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
print("–ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...")
trainer.train()
print("–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
model.save_pretrained("./data/models/support-bot-lora")
tokenizer.save_pretrained("./data/models/support-bot-lora")
```

**3. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –¥–æ–æ–±—É—á–µ–Ω–∏–µ:**
```bash
python train_model.py
```

## üìä –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏

### üß™ –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞

**1. –°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª `test_questions.json`:**
```json
{
  "test_cases": [
    {
      "question": "–ó–∞–±—ã–ª –ø–∞—Ä–æ–ª—å –æ—Ç –∞–∫–∫–∞—É–Ω—Ç–∞",
      "expected_keywords": ["—Å–±—Ä–æ—Å", "–ø–∞—Ä–æ–ª—å", "email", "–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏"],
      "category": "account"
    },
    {
      "question": "–ù–µ –ø—Ä–∏—Ö–æ–¥–∏—Ç –ø–∏—Å—å–º–æ —Å –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ–º",
      "expected_keywords": ["—Å–ø–∞–º", "–ø—Ä–æ–≤–µ—Ä—å—Ç–µ", "–ø–∞–ø–∫–∞", "email"],
      "category": "technical"
    }
  ]
}
```

**2. –°–æ–∑–¥–∞–π—Ç–µ —Å–∫—Ä–∏–ø—Ç —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è `test_model.py`:**
```python
import json
import requests

def test_model(question, expected_keywords):
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º –≤–æ–ø—Ä–æ—Å–µ"""
    url = "http://localhost:11434/api/generate"
    
    data = {
        "model": "support-bot-v2",
        "prompt": f"–í–æ–ø—Ä–æ—Å –∫–ª–∏–µ–Ω—Ç–∞: {question}\n–û—Ç–≤–µ—Ç:",
        "stream": False
    }
    
    response = requests.post(url, json=data)
    result = response.json()
    answer = result["response"]
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
    found_keywords = []
    for keyword in expected_keywords:
        if keyword.lower() in answer.lower():
            found_keywords.append(keyword)
    
    score = len(found_keywords) / len(expected_keywords)
    
    return {
        "question": question,
        "answer": answer,
        "found_keywords": found_keywords,
        "score": score
    }

# –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
with open("test_questions.json", "r", encoding="utf-8") as f:
    test_data = json.load(f)

# –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤
results = []
for test_case in test_data["test_cases"]:
    result = test_model(
        test_case["question"], 
        test_case["expected_keywords"]
    )
    results.append(result)

# –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
total_score = sum(r["score"] for r in results) / len(results)
print(f"–û–±—â–∏–π –±–∞–ª–ª –º–æ–¥–µ–ª–∏: {total_score:.2f}")

for result in results:
    print(f"\n–í–æ–ø—Ä–æ—Å: {result['question']}")
    print(f"–û—Ç–≤–µ—Ç: {result['answer']}")
    print(f"–ù–∞–π–¥–µ–Ω–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {result['found_keywords']}")
    print(f"–ë–∞–ª–ª: {result['score']:.2f}")
```

## üîÑ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –¥–æ–æ–±—É—á–µ–Ω–∏–µ

### üìÖ –°–æ–∑–¥–∞–π—Ç–µ —Å–∫—Ä–∏–ø—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è `auto_retrain.py`:

```python
import json
import os
import subprocess
from datetime import datetime

def collect_new_data():
    """–°–æ–±–∏—Ä–∞–µ—Ç –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –ª–æ–≥–æ–≤"""
    log_file = "data/logs/chat_log.json"
    
    if not os.path.exists(log_file):
        return []
    
    with open(log_file, "r", encoding="utf-8") as f:
        logs = json.load(f)
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –¥–∏–∞–ª–æ–≥–∏
    quality_conversations = []
    for log in logs:
        if len(log["answer"]) > 50 and "–æ—à–∏–±–∫–∞" not in log["answer"].lower():
            quality_conversations.append({
                "question": log["question"],
                "answer": log["answer"],
                "timestamp": log["timestamp"]
            })
    
    return quality_conversations

def update_training_data(new_conversations):
    """–û–±–Ω–æ–≤–ª—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
    training_file = "data/training/support_conversations.json"
    
    if os.path.exists(training_file):
        with open(training_file, "r", encoding="utf-8") as f:
            data = json.load(f)
    else:
        data = {"conversations": []}
    
    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –¥–∏–∞–ª–æ–≥–∏
    for conv in new_conversations:
        data["conversations"].append(conv)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    with open(training_file, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    return len(new_conversations)

def retrain_model():
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –¥–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
    print("–ù–∞—á–∏–Ω–∞–µ–º –¥–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    
    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å
    subprocess.run(["ollama", "create", "support-bot-v3", "-f", "Modelfile"])
    
    print("–î–æ–æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    return True

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–æ–æ–±—É—á–µ–Ω–∏—è"""
    print(f"–ó–∞–ø—É—Å–∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–æ–æ–±—É—á–µ–Ω–∏—è: {datetime.now()}")
    
    # –°–æ–±–∏—Ä–∞–µ–º –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    new_data = collect_new_data()
    print(f"–ù–∞–π–¥–µ–Ω–æ {len(new_data)} –Ω–æ–≤—ã—Ö –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤")
    
    if len(new_data) >= 5:  # –î–æ–æ–±—É—á–∞–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö
        # –û–±–Ω–æ–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        updated_count = update_training_data(new_data)
        print(f"–î–æ–±–∞–≤–ª–µ–Ω–æ {updated_count} –Ω–æ–≤—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤ –≤ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –¥–æ–æ–±—É—á–µ–Ω–∏–µ
        retrain_model()
        
        print("–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –¥–æ–æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
    else:
        print("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è (–º–∏–Ω–∏–º—É–º 5 –¥–∏–∞–ª–æ–≥–æ–≤)")

if __name__ == "__main__":
    main()
```

## üéØ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –æ–±—É—á–µ–Ω–∏—é

### ‚úÖ –õ—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏:

1. **–ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö –≤–∞–∂–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞**
   - –õ—É—á—à–µ 50 –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤, —á–µ–º 500 –ø–ª–æ—Ö–∏—Ö
   - –ü—Ä–æ–≤–µ—Ä—è–π—Ç–µ –æ—Ç–≤–µ—Ç—ã –ø–µ—Ä–µ–¥ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º –≤ –æ–±—É—á–µ–Ω–∏–µ

2. **–†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –≤–æ–ø—Ä–æ—Å–æ–≤**
   - –î–æ–±–∞–≤–ª—è–π—Ç–µ –≤–æ–ø—Ä–æ—Å—ã –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
   - –í–∫–ª—é—á–∞–π—Ç–µ —Å–∏–Ω–æ–Ω–∏–º—ã –∏ —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏

3. **–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã**
   - –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —á–µ—Ç–∫—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
   - –î–æ–±–∞–≤–ª—è–π—Ç–µ –ø–æ—à–∞–≥–æ–≤—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
   - –í–∫–ª—é—á–∞–π—Ç–µ –ø—Ä–∏–º–µ—Ä—ã

4. **–†–µ–≥—É–ª—è—Ä–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ**
   - –î–æ–æ–±—É—á–∞–π—Ç–µ –º–æ–¥–µ–ª—å –∫–∞–∂–¥—É—é –Ω–µ–¥–µ–ª—é
   - –ê–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –ª–æ–≥–∏ –∏ —É–ª—É—á—à–∞–π—Ç–µ –æ—Ç–≤–µ—Ç—ã

5. **–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ**
   - –í—Å–µ–≥–¥–∞ —Ç–µ—Å—Ç–∏—Ä—É–π—Ç–µ –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å
   - –°—Ä–∞–≤–Ω–∏–≤–∞–π—Ç–µ —Å –ø—Ä–µ–¥—ã–¥—É—â–µ–π –≤–µ—Ä—Å–∏–µ–π
   - –°–æ–±–∏—Ä–∞–π—Ç–µ –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å

### üìà –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞:

- **–¢–æ—á–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–æ–≤** - —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –æ–∂–∏–¥–∞–µ–º–æ–º—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É
- **–ü–æ–ª–Ω–æ—Ç–∞** - –ø–æ–∫—Ä—ã—Ç–∏–µ –≤—Å–µ—Ö –∞—Å–ø–µ–∫—Ç–æ–≤ –≤–æ–ø—Ä–æ—Å–∞
- **–ü–æ–Ω—è—Ç–Ω–æ—Å—Ç—å** - –ª–µ–≥–∫–æ—Å—Ç—å –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –æ—Ç–≤–µ—Ç–∞
- **–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª–∏–∑–º** - –≤–µ–∂–ª–∏–≤–æ—Å—Ç—å –∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å

## üöÄ –ó–∞–ø—É—Å–∫ —Å–∏—Å—Ç–µ–º—ã

### 1. –ó–∞–ø—É—Å—Ç–∏—Ç–µ Ollama:
```bash
ollama serve
```

### 2. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ:
```bash
python app_simple_ollama.py
```

### 3. –û—Ç–∫—Ä–æ–π—Ç–µ –±—Ä–∞—É–∑–µ—Ä:
```
http://localhost:7860
```

### 4. –ù–∞—á–Ω–∏—Ç–µ –æ–±—É—á–µ–Ω–∏–µ:
- –î–æ–±–∞–≤–ª—è–π—Ç–µ FAQ —á–µ—Ä–µ–∑ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
- –¢–µ—Å—Ç–∏—Ä—É–π—Ç–µ –æ—Ç–≤–µ—Ç—ã –±–æ—Ç–∞
- –ê–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –ª–æ–≥–∏
- –î–æ–æ–±—É—á–∞–π—Ç–µ –º–æ–¥–µ–ª—å

---
**–¢–µ–ø–µ—Ä—å —É –≤–∞—Å –µ—Å—Ç—å –ø–æ–ª–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏! üéâ**
