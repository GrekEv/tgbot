# ü¶ô –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Ollama –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ RAG Support Bot

## üéØ –¶–µ–ª—å
–ù–∞—Å—Ç—Ä–æ–∏—Ç—å –ª–æ–∫–∞–ª—å–Ω—É—é Ollama –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å RAG Support Bot –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–æ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π.

## üìã –ß—Ç–æ –Ω—É–∂–Ω–æ

1. **Windows 10/11** —Å –º–∏–Ω–∏–º—É–º 8GB RAM
2. **Python 3.8+**
3. **Git** (–¥–ª—è –∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è Ollama)
4. **CUDA** (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –¥–ª—è GPU —É—Å–∫–æ—Ä–µ–Ω–∏—è)

## üöÄ –ü–æ—à–∞–≥–æ–≤–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞

### –®–∞–≥ 1: –£—Å—Ç–∞–Ω–æ–≤–∫–∞ Ollama

**–î–ª—è Windows:**
```bash
# –°–∫–∞—á–∞–π—Ç–µ —É—Å—Ç–∞–Ω–æ–≤—â–∏–∫ —Å https://ollama.com/download
# –ò–ª–∏ —á–µ—Ä–µ–∑ PowerShell:
winget install Ollama.Ollama
```

**–î–ª—è Linux/Mac:**
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

### –®–∞–≥ 2: –ó–∞–ø—É—Å–∫ Ollama —Å–µ—Ä–≤–µ—Ä–∞

```bash
# –ó–∞–ø—É—Å—Ç–∏—Ç–µ Ollama –≤ —Ñ–æ–Ω–æ–≤–æ–º —Ä–µ–∂–∏–º–µ
ollama serve
```

Ollama –±—É–¥–µ—Ç –¥–æ—Å—Ç—É–ø–µ–Ω –ø–æ –∞–¥—Ä–µ—Å—É: `http://localhost:11434`

### –®–∞–≥ 3: –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –º–æ–¥–µ–ª–µ–π

**–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏:**

```bash
# –ù–µ–±–æ–ª—å—à–∞—è –±—ã—Å—Ç—Ä–∞—è –º–æ–¥–µ–ª—å (2GB)
ollama pull llama3.2:1b

# –°—Ä–µ–¥–Ω—è—è –º–æ–¥–µ–ª—å (4GB) - —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è
ollama pull llama3.2:3b

# –ë–æ–ª—å—à–∞—è –º–æ–¥–µ–ª—å (8GB) - –ª—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ
ollama pull llama3.2:8b

# –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
ollama pull saiga3:8b

# –ú–æ–¥–µ–ª—å –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è
ollama pull llama3.2:8b
```

### –®–∞–≥ 4: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è

–°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª `.env`:

```env
# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è Ollama
USE_DEEPSEEK=false
OLLAMA_HOST=http://localhost:11434
MODEL_NAME=llama3.2:8b

# –ü–æ—Ä—Ç –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
PORT=7860

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è
TRAINING_DATA_PATH=./data/training/
MODEL_OUTPUT_PATH=./models/
```

### –®–∞–≥ 5: –ú–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—è app.py –¥–ª—è Ollama

–°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª `app_ollama.py`:

```python
import os
import json
import requests
import gradio as gr
from datetime import datetime

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ Ollama
OLLAMA_HOST = os.getenv("OLLAMA_HOST", "http://localhost:11434")
MODEL_NAME = os.getenv("MODEL_NAME", "llama3.2:8b")

def call_ollama_api(question, context=""):
    """–í—ã–∑—ã–≤–∞–µ—Ç –ª–æ–∫–∞–ª—å–Ω—É—é Ollama API"""
    try:
        url = f"{OLLAMA_HOST}/api/generate"
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
        if context:
            prompt = f"""–¢—ã –æ–ø–µ—Ä–∞—Ç–æ—Ä —Å–ª—É–∂–±—ã –ø–æ–¥–¥–µ—Ä–∂–∫–∏. –ù–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –æ—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –∫–ª–∏–µ–Ω—Ç–∞.

–ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}

–í–æ–ø—Ä–æ—Å –∫–ª–∏–µ–Ω—Ç–∞: {question}

–û—Ç–≤–µ—Ç—å –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ –∏ –≤–µ–∂–ª–∏–≤–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ:"""
        else:
            prompt = f"–¢—ã –æ–ø–µ—Ä–∞—Ç–æ—Ä —Å–ª—É–∂–±—ã –ø–æ–¥–¥–µ—Ä–∂–∫–∏. –û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –∫–ª–∏–µ–Ω—Ç–∞: {question}"
        
        data = {
            "model": MODEL_NAME,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": 0.7,
                "top_p": 0.9,
                "max_tokens": 500
            }
        }
        
        response = requests.post(url, json=data, timeout=60)
        response.raise_for_status()
        
        result = response.json()
        return result["response"]
        
    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ Ollama: {str(e)}"

# –û—Å—Ç–∞–ª—å–Ω–æ–π –∫–æ–¥ –∞–Ω–∞–ª–æ–≥–∏—á–µ–Ω app.py...
```

## üéì –î–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π

### –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è

1. **–°–æ–∑–¥–∞–π—Ç–µ –ø–∞–ø–∫—É –¥–ª—è –¥–∞–Ω–Ω—ã—Ö:**
```bash
mkdir -p data/training
```

2. **–°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏ –¥–∏–∞–ª–æ–≥–æ–≤:**
```json
{
  "conversations": [
    {
      "question": "–ö–∞–∫ —Å–±—Ä–æ—Å–∏—Ç—å –ø–∞—Ä–æ–ª—å?",
      "answer": "–î–ª—è —Å–±—Ä–æ—Å–∞ –ø–∞—Ä–æ–ª—è –ø–µ—Ä–µ–π–¥–∏—Ç–µ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É –≤—Ö–æ–¥–∞ –∏ –Ω–∞–∂–º–∏—Ç–µ '–ó–∞–±—ã–ª–∏ –ø–∞—Ä–æ–ª—å?'. –í–≤–µ–¥–∏—Ç–µ –≤–∞—à email –∏ —Å–ª–µ–¥—É–π—Ç–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –≤ –ø–∏—Å—å–º–µ."
    },
    {
      "question": "–ù–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –æ–ø–ª–∞—Ç–∞",
      "answer": "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–∞–Ω–Ω—ã–µ –∫–∞—Ä—Ç—ã –∏ –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥–æ–π —Å–ø–æ—Å–æ–± –æ–ø–ª–∞—Ç—ã. –ï—Å–ª–∏ –ø—Ä–æ–±–ª–µ–º–∞ –æ—Å—Ç–∞–µ—Ç—Å—è, –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –≤ –±–∞–Ω–∫."
    }
  ]
}
```

### –°–æ–∑–¥–∞–Ω–∏–µ –∫–∞—Å—Ç–æ–º–Ω–æ–π –º–æ–¥–µ–ª–∏

1. **–°–æ–∑–¥–∞–π—Ç–µ Modelfile:**
```dockerfile
FROM llama3.2:8b

# –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
SYSTEM """–¢—ã –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –æ–ø–µ—Ä–∞—Ç–æ—Ä —Å–ª—É–∂–±—ã –ø–æ–¥–¥–µ—Ä–∂–∫–∏. 
–¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –ø–æ–º–æ–≥–∞—Ç—å –∫–ª–∏–µ–Ω—Ç–∞–º —Ä–µ—à–∞—Ç—å –∏—Ö –ø—Ä–æ–±–ª–µ–º—ã –≤–µ–∂–ª–∏–≤–æ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ.
–ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤."""

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER max_tokens 500
```

2. **–°–æ–∑–¥–∞–π—Ç–µ –º–æ–¥–µ–ª—å:**
```bash
ollama create support-bot -f Modelfile
```

3. **–ó–∞–ø—É—Å—Ç–∏—Ç–µ –∫–∞—Å—Ç–æ–º–Ω—É—é –º–æ–¥–µ–ª—å:**
```bash
ollama run support-bot
```

### –î–æ–æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é LoRA

1. **–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è:**
```bash
pip install transformers datasets accelerate peft
```

2. **–°–æ–∑–¥–∞–π—Ç–µ —Å–∫—Ä–∏–ø—Ç –¥–æ–æ–±—É—á–µ–Ω–∏—è:**
```python
# train_lora.py
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import LoraConfig, get_peft_model
import torch

# –ó–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å
model_name = "meta-llama/Llama-3.2-8B"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LoRA
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)

# –î–æ–æ–±—É—á–µ–Ω–∏–µ...
```

## üîß –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –ø–∞–Ω–µ–ª—å—é –ø–æ–¥–¥–µ—Ä–∂–∫–∏

### –ú–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—è support_dashboard.html

–î–æ–±–∞–≤—å—Ç–µ –≤ JavaScript —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Ollama:

```javascript
async function callOllamaAPI(question, context) {
    try {
        const response = await fetch('http://localhost:11434/api/generate', {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json'
            },
            body: JSON.stringify({
                model: 'support-bot',
                prompt: `–ö–æ–Ω—Ç–µ–∫—Å—Ç: ${context}\n\n–í–æ–ø—Ä–æ—Å: ${question}\n\n–û—Ç–≤–µ—Ç:`,
                stream: false,
                options: {
                    temperature: 0.7,
                    max_tokens: 500
                }
            })
        });
        
        const data = await response.json();
        return data.response;
    } catch (error) {
        return `–û—à–∏–±–∫–∞: ${error.message}`;
    }
}
```

## üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è

### –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç—ã Ollama

```bash
# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞
curl http://localhost:11434/api/tags

# –¢–µ—Å—Ç –º–æ–¥–µ–ª–∏
ollama run llama3.2:8b "–ü—Ä–∏–≤–µ—Ç, –∫–∞–∫ –¥–µ–ª–∞?"

# –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ä–µ—Å—É—Ä—Å–æ–≤
ollama ps
```

### –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

1. **–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU:**
```bash
# –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ CUDA –¥–ª—è Windows
# Ollama –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç GPU –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
```

2. **–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞–º—è—Ç–∏:**
```bash
# –í .env —Ñ–∞–π–ª–µ
OLLAMA_NUM_PARALLEL=2
OLLAMA_MAX_LOADED_MODELS=1
```

## üöÄ –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã

1. **–ó–∞–ø—É—Å—Ç–∏—Ç–µ Ollama:**
```bash
ollama serve
```

2. **–ó–∞–ø—É—Å—Ç–∏—Ç–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ:**
```bash
python app_ollama.py
```

3. **–û—Ç–∫—Ä–æ–π—Ç–µ –ø–∞–Ω–µ–ª—å –ø–æ–¥–¥–µ—Ä–∂–∫–∏:**
```bash
start support_dashboard.html
```

## üéØ –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –ª–æ–∫–∞–ª—å–Ω–æ–π Ollama

- ‚úÖ **–ü–æ–ª–Ω–∞—è –ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç—å** - –¥–∞–Ω–Ω—ã–µ –Ω–µ –ø–æ–∫–∏–¥–∞—é—Ç –≤–∞—à –∫–æ–º–ø—å—é—Ç–µ—Ä
- ‚úÖ **–ë–µ—Å–ø–ª–∞—Ç–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ** - –Ω–µ—Ç –ª–∏–º–∏—Ç–æ–≤ API
- ‚úÖ **–î–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π** - –º–æ–∂–Ω–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ–¥ –≤–∞—à–∏ –∑–∞–¥–∞—á–∏
- ‚úÖ **–û—Ñ–ª–∞–π–Ω —Ä–∞–±–æ—Ç–∞** - –Ω–µ –Ω—É–∂–µ–Ω –∏–Ω—Ç–µ—Ä–Ω–µ—Ç
- ‚úÖ **–ë—ã—Å—Ç—Ä—ã–µ –æ—Ç–≤–µ—Ç—ã** - –Ω–µ—Ç –∑–∞–¥–µ—Ä–∂–µ–∫ —Å–µ—Ç–∏

## üîß –†–µ—à–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º

**–ü—Ä–æ–±–ª–µ–º–∞:** Ollama –Ω–µ –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è
```bash
# –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ —Å–ª—É–∂–±—É
ollama serve
```

**–ü—Ä–æ–±–ª–µ–º–∞:** –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è
```bash
# –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–æ—Å—Ç—É–ø–Ω—É—é –ø–∞–º—è—Ç—å
ollama ps
# –£–¥–∞–ª–∏—Ç–µ –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –º–æ–¥–µ–ª–∏
ollama rm model_name
```

**–ü—Ä–æ–±–ª–µ–º–∞:** –ú–µ–¥–ª–µ–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã
```bash
# –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–µ–Ω—å—à—É—é –º–æ–¥–µ–ª—å
ollama pull llama3.2:3b
```

---
**–ì–æ—Ç–æ–≤–æ! –¢–µ–ø–µ—Ä—å —É –≤–∞—Å –µ—Å—Ç—å –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–∞—è –ª–æ–∫–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø–æ–¥–¥–µ—Ä–∂–∫–∏ —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –¥–æ–æ–±—É—á–µ–Ω–∏—è! üéâ**
