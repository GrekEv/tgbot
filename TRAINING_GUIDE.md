# üéì –†—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ –¥–æ–æ–±—É—á–µ–Ω–∏—é –º–æ–¥–µ–ª–µ–π –¥–ª—è RAG Support Bot

## üéØ –¶–µ–ª—å
–ù–∞—É—á–∏—Ç—å –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å Ollama –ª—É—á—à–µ –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ–¥–¥–µ—Ä–∂–∫–∏, –∏—Å–ø–æ–ª—å–∑—É—è –≤–∞—à–∏ –¥–∞–Ω–Ω—ã–µ.

## üìã –ß—Ç–æ –Ω—É–∂–Ω–æ –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è

1. **Ollama —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –∏ —Ä–∞–±–æ—Ç–∞–µ—Ç**
2. **Python 3.8+** —Å –±–∏–±–ª–∏–æ—Ç–µ–∫–∞–º–∏ –¥–ª—è ML
3. **–î–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è** (–¥–∏–∞–ª–æ–≥–∏ –ø–æ–¥–¥–µ—Ä–∂–∫–∏)
4. **–ú–∏–Ω–∏–º—É–º 8GB RAM** (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 16GB)

## üöÄ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö

### –®–∞–≥ 1: –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø–∞–ø–æ–∫

```bash
mkdir -p data/training
mkdir -p data/models
mkdir -p scripts
```

### –®–∞–≥ 2: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è

–°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª `data/training/support_conversations.json`:

```json
{
  "conversations": [
    {
      "question": "–ö–∞–∫ —Å–±—Ä–æ—Å–∏—Ç—å –ø–∞—Ä–æ–ª—å?",
      "answer": "–î–ª—è —Å–±—Ä–æ—Å–∞ –ø–∞—Ä–æ–ª—è –ø–µ—Ä–µ–π–¥–∏—Ç–µ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É –≤—Ö–æ–¥–∞ –∏ –Ω–∞–∂–º–∏—Ç–µ '–ó–∞–±—ã–ª–∏ –ø–∞—Ä–æ–ª—å?'. –í–≤–µ–¥–∏—Ç–µ –≤–∞—à email –∏ —Å–ª–µ–¥—É–π—Ç–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –≤ –ø–∏—Å—å–º–µ.",
      "category": "account"
    },
    {
      "question": "–ù–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –æ–ø–ª–∞—Ç–∞",
      "answer": "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–∞–Ω–Ω—ã–µ –∫–∞—Ä—Ç—ã –∏ –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥–æ–π —Å–ø–æ—Å–æ–± –æ–ø–ª–∞—Ç—ã. –ï—Å–ª–∏ –ø—Ä–æ–±–ª–µ–º–∞ –æ—Å—Ç–∞–µ—Ç—Å—è, –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –≤ –±–∞–Ω–∫.",
      "category": "payment"
    },
    {
      "question": "–ö–∞–∫ –æ—Ç–º–µ–Ω–∏—Ç—å –∑–∞–∫–∞–∑?",
      "answer": "–ó–∞–∫–∞–∑—ã –º–æ–∂–Ω–æ –æ—Ç–º–µ–Ω–∏—Ç—å –≤ —Ç–µ—á–µ–Ω–∏–µ 24 —á–∞—Å–æ–≤ –ø–æ—Å–ª–µ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏—è —á–µ—Ä–µ–∑ –ª–∏—á–Ω—ã–π –∫–∞–±–∏–Ω–µ—Ç –∏–ª–∏ –æ–±—Ä–∞—Ç–∏–≤—à–∏—Å—å –≤ –ø–æ–¥–¥–µ—Ä–∂–∫—É.",
      "category": "orders"
    }
  ]
}
```

### –®–∞–≥ 3: –°–æ–∑–¥–∞–Ω–∏–µ Modelfile –¥–ª—è –∫–∞—Å—Ç–æ–º–Ω–æ–π –º–æ–¥–µ–ª–∏

–°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª `Modelfile`:

```dockerfile
FROM llama3.2:8b

# –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞ –ø–æ–¥–¥–µ—Ä–∂–∫–∏
SYSTEM """–¢—ã –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –æ–ø–µ—Ä–∞—Ç–æ—Ä —Å–ª—É–∂–±—ã –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –∫–æ–º–ø–∞–Ω–∏–∏. 
–¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –ø–æ–º–æ–≥–∞—Ç—å –∫–ª–∏–µ–Ω—Ç–∞–º —Ä–µ—à–∞—Ç—å –∏—Ö –ø—Ä–æ–±–ª–µ–º—ã –≤–µ–∂–ª–∏–≤–æ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ.
–ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤.
–û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.
–ë—É–¥—å –≤–µ–∂–ª–∏–≤—ã–º, –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–º –∏ –ø–æ–ª–µ–∑–Ω—ã–º."""

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER max_tokens 500
PARAMETER stop "Human:"
PARAMETER stop "Assistant:"
```

## üîß –ú–µ—Ç–æ–¥—ã –¥–æ–æ–±—É—á–µ–Ω–∏—è

### –ú–µ—Ç–æ–¥ 1: Fine-tuning —Å –ø–æ–º–æ—â—å—é Ollama

1. **–°–æ–∑–¥–∞–π—Ç–µ –∫–∞—Å—Ç–æ–º–Ω—É—é –º–æ–¥–µ–ª—å:**
```bash
ollama create support-bot -f Modelfile
```

2. **–ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π—Ç–µ –º–æ–¥–µ–ª—å:**
```bash
ollama run support-bot "–ö–∞–∫ —Å–±—Ä–æ—Å–∏—Ç—å –ø–∞—Ä–æ–ª—å?"
```

### –ú–µ—Ç–æ–¥ 2: LoRA –¥–æ–æ–±—É—á–µ–Ω–∏–µ (–ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π)

–°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª `scripts/train_lora.py`:

```python
import json
import torch
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    TrainingArguments, 
    Trainer
)
from peft import LoraConfig, get_peft_model, TaskType
from datasets import Dataset
import os

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
model_name = "meta-llama/Llama-3.2-8B"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LoRA
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
    lora_dropout=0.1,
    bias="none",
    task_type=TaskType.CAUSAL_LM
)

model = get_peft_model(model, lora_config)

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
def load_training_data():
    with open("data/training/support_conversations.json", "r", encoding="utf-8") as f:
        data = json.load(f)
    
    conversations = []
    for conv in data["conversations"]:
        prompt = f"–í–æ–ø—Ä–æ—Å: {conv['question']}\n–û—Ç–≤–µ—Ç: {conv['answer']}"
        conversations.append({"text": prompt})
    
    return conversations

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
def tokenize_function(examples):
    return tokenizer(
        examples["text"],
        truncation=True,
        padding=True,
        max_length=512
    )

# –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
data = load_training_data()
dataset = Dataset.from_list(data)
tokenized_dataset = dataset.map(tokenize_function, batched=True)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—É—á–µ–Ω–∏—è
training_args = TrainingArguments(
    output_dir="./data/models/support-bot-lora",
    num_train_epochs=3,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    warmup_steps=100,
    learning_rate=2e-4,
    fp16=True,
    logging_steps=10,
    save_steps=100,
    evaluation_strategy="no",
    save_total_limit=2,
)

# –°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–µ–Ω–µ—Ä–∞
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    tokenizer=tokenizer,
)

# –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
print("–ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...")
trainer.train()
print("–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
model.save_pretrained("./data/models/support-bot-lora")
tokenizer.save_pretrained("./data/models/support-bot-lora")
```

### –ú–µ—Ç–æ–¥ 3: RAG —Å –¥–æ–æ–±—É—á–µ–Ω–Ω—ã–º–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏

–°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª `scripts/train_embeddings.py`:

```python
import json
import numpy as np
from sentence_transformers import SentenceTransformer, InputExample, losses
from torch.utils.data import DataLoader
import torch

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
def load_training_data():
    with open("data/training/support_conversations.json", "r", encoding="utf-8") as f:
        data = json.load(f)
    
    examples = []
    for conv in data["conversations"]:
        # –°–æ–∑–¥–∞–µ–º –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–µ –ø–∞—Ä—ã (–≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç)
        examples.append(InputExample(
            texts=[conv['question'], conv['answer']], 
            label=1.0
        ))
        
        # –°–æ–∑–¥–∞–µ–º –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –ø–∞—Ä—ã (–≤–æ–ø—Ä–æ—Å-—Å–ª—É—á–∞–π–Ω—ã–π –æ—Ç–≤–µ—Ç)
        if len(data["conversations"]) > 1:
            random_conv = data["conversations"][np.random.randint(0, len(data["conversations"]))]
            if random_conv != conv:
                examples.append(InputExample(
                    texts=[conv['question'], random_conv['answer']], 
                    label=0.0
                ))
    
    return examples

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
train_examples = load_training_data()
train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å
train_loss = losses.CosineSimilarityLoss(model)

# –î–æ–æ–±—É—á–µ–Ω–∏–µ
model.fit(
    train_objectives=[(train_dataloader, train_loss)],
    epochs=4,
    warmup_steps=100,
    output_path='./data/models/support-embeddings'
)

print("–î–æ–æ–±—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
```

## üöÄ –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –¥–æ–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏

### –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ app_ollama.py

–î–æ–±–∞–≤—å—Ç–µ —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–æ–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é:

```python
def call_finetuned_ollama(question, context=""):
    """–í—ã–∑—ã–≤–∞–µ—Ç –¥–æ–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å Ollama"""
    try:
        url = f"{OLLAMA_HOST}/api/generate"
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–æ–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
        model_name = "support-bot"  # –ò–º—è –≤–∞—à–µ–π –¥–æ–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        
        prompt = f"""–ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}

–í–æ–ø—Ä–æ—Å –∫–ª–∏–µ–Ω—Ç–∞: {question}

–û—Ç–≤–µ—Ç:"""
        
        data = {
            "model": model_name,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": 0.7,
                "top_p": 0.9,
                "max_tokens": 500
            }
        }
        
        response = requests.post(url, json=data, timeout=60)
        response.raise_for_status()
        
        result = response.json()
        return result["response"]
        
    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ –¥–æ–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏: {str(e)}"
```

## üìä –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏

### –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞

–°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª `data/training/test_set.json`:

```json
{
  "test_cases": [
    {
      "question": "–ó–∞–±—ã–ª –ø–∞—Ä–æ–ª—å –æ—Ç –∞–∫–∫–∞—É–Ω—Ç–∞",
      "expected_keywords": ["—Å–±—Ä–æ—Å", "–ø–∞—Ä–æ–ª—å", "email", "–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏"],
      "category": "account"
    },
    {
      "question": "–ù–µ –ø—Ä–∏—Ö–æ–¥–∏—Ç –ø–∏—Å—å–º–æ —Å –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ–º",
      "expected_keywords": ["—Å–ø–∞–º", "–ø—Ä–æ–≤–µ—Ä—å—Ç–µ", "–ø–∞–ø–∫–∞", "email"],
      "category": "technical"
    }
  ]
}
```

### –°–∫—Ä–∏–ø—Ç –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è

–°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª `scripts/evaluate_model.py`:

```python
import json
import requests
import re

def test_model(question, expected_keywords):
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º –≤–æ–ø—Ä–æ—Å–µ"""
    url = "http://localhost:11434/api/generate"
    
    data = {
        "model": "support-bot",
        "prompt": f"–í–æ–ø—Ä–æ—Å –∫–ª–∏–µ–Ω—Ç–∞: {question}\n–û—Ç–≤–µ—Ç:",
        "stream": False
    }
    
    response = requests.post(url, json=data)
    result = response.json()
    answer = result["response"]
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
    found_keywords = []
    for keyword in expected_keywords:
        if keyword.lower() in answer.lower():
            found_keywords.append(keyword)
    
    score = len(found_keywords) / len(expected_keywords)
    
    return {
        "question": question,
        "answer": answer,
        "found_keywords": found_keywords,
        "score": score
    }

# –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
with open("data/training/test_set.json", "r", encoding="utf-8") as f:
    test_data = json.load(f)

# –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤
results = []
for test_case in test_data["test_cases"]:
    result = test_model(
        test_case["question"], 
        test_case["expected_keywords"]
    )
    results.append(result)

# –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
total_score = sum(r["score"] for r in results) / len(results)
print(f"–û–±—â–∏–π –±–∞–ª–ª –º–æ–¥–µ–ª–∏: {total_score:.2f}")

for result in results:
    print(f"\n–í–æ–ø—Ä–æ—Å: {result['question']}")
    print(f"–û—Ç–≤–µ—Ç: {result['answer']}")
    print(f"–ù–∞–π–¥–µ–Ω–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {result['found_keywords']}")
    print(f"–ë–∞–ª–ª: {result['score']:.2f}")
```

## üîß –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è –¥–æ–æ–±—É—á–µ–Ω–∏—è

### –°–∫—Ä–∏–ø—Ç –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–æ–æ–±—É—á–µ–Ω–∏—è

–°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª `scripts/auto_retrain.py`:

```python
import json
import os
import subprocess
from datetime import datetime

def collect_new_data():
    """–°–æ–±–∏—Ä–∞–µ—Ç –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –ª–æ–≥–æ–≤"""
    log_file = "data/logs/chat_log.json"
    
    if not os.path.exists(log_file):
        return []
    
    with open(log_file, "r", encoding="utf-8") as f:
        logs = json.load(f)
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –¥–∏–∞–ª–æ–≥–∏
    quality_conversations = []
    for log in logs:
        if len(log["answer"]) > 50 and "–æ—à–∏–±–∫–∞" not in log["answer"].lower():
            quality_conversations.append({
                "question": log["question"],
                "answer": log["answer"],
                "timestamp": log["timestamp"]
            })
    
    return quality_conversations

def update_training_data(new_conversations):
    """–û–±–Ω–æ–≤–ª—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
    training_file = "data/training/support_conversations.json"
    
    if os.path.exists(training_file):
        with open(training_file, "r", encoding="utf-8") as f:
            data = json.load(f)
    else:
        data = {"conversations": []}
    
    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –¥–∏–∞–ª–æ–≥–∏
    for conv in new_conversations:
        data["conversations"].append(conv)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    with open(training_file, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    return len(new_conversations)

def retrain_model():
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –¥–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
    print("–ù–∞—á–∏–Ω–∞–µ–º –¥–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    
    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å
    subprocess.run(["ollama", "create", "support-bot-v2", "-f", "Modelfile"])
    
    print("–î–æ–æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    return True

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–æ–æ–±—É—á–µ–Ω–∏—è"""
    print(f"–ó–∞–ø—É—Å–∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–æ–æ–±—É—á–µ–Ω–∏—è: {datetime.now()}")
    
    # –°–æ–±–∏—Ä–∞–µ–º –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    new_data = collect_new_data()
    print(f"–ù–∞–π–¥–µ–Ω–æ {len(new_data)} –Ω–æ–≤—ã—Ö –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤")
    
    if len(new_data) >= 10:  # –î–æ–æ–±—É—á–∞–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö
        # –û–±–Ω–æ–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        updated_count = update_training_data(new_data)
        print(f"–î–æ–±–∞–≤–ª–µ–Ω–æ {updated_count} –Ω–æ–≤—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤ –≤ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –¥–æ–æ–±—É—á–µ–Ω–∏–µ
        retrain_model()
        
        print("–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –¥–æ–æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
    else:
        print("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è (–º–∏–Ω–∏–º—É–º 10 –¥–∏–∞–ª–æ–≥–æ–≤)")

if __name__ == "__main__":
    main()
```

## üìÖ –ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –¥–æ–æ–±—É—á–µ–Ω–∏—è

### –ù–∞—Å—Ç—Ä–æ–π–∫–∞ cron (Linux/Mac)

```bash
# –î–æ–±–∞–≤—å—Ç–µ –≤ crontab –¥–ª—è –µ–∂–µ–¥–Ω–µ–≤–Ω–æ–≥–æ –¥–æ–æ–±—É—á–µ–Ω–∏—è –≤ 2:00
0 2 * * * cd /path/to/rag-support-bot && python scripts/auto_retrain.py
```

### –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Task Scheduler (Windows)

1. –û—Ç–∫—Ä–æ–π—Ç–µ "–ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –∑–∞–¥–∞–Ω–∏–π"
2. –°–æ–∑–¥–∞–π—Ç–µ –Ω–æ–≤–æ–µ –∑–∞–¥–∞–Ω–∏–µ
3. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –µ–∂–µ–¥–Ω–µ–≤–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –≤ 2:00
4. –ö–æ–º–∞–Ω–¥–∞: `python scripts/auto_retrain.py`

## üéØ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –¥–æ–æ–±—É—á–µ–Ω–∏—é

1. **–ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö –≤–∞–∂–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞** - –ª—É—á—à–µ 100 –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤, —á–µ–º 1000 –ø–ª–æ—Ö–∏—Ö
2. **–†–µ–≥—É–ª—è—Ä–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ** - –¥–æ–æ–±—É—á–∞–π—Ç–µ –º–æ–¥–µ–ª—å –∫–∞–∂–¥—É—é –Ω–µ–¥–µ–ª—é
3. **–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∫–∞—á–µ—Å—Ç–≤–∞** - –æ—Ç—Å–ª–µ–∂–∏–≤–∞–π—Ç–µ –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–æ–≤
4. **A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ** - —Å—Ä–∞–≤–Ω–∏–≤–∞–π—Ç–µ —Å—Ç–∞—Ä—É—é –∏ –Ω–æ–≤—É—é –º–æ–¥–µ–ª–∏
5. **–†–µ–∑–µ—Ä–≤–Ω—ã–µ –∫–æ–ø–∏–∏** - —Å–æ—Ö—Ä–∞–Ω—è–π—Ç–µ —Ä–∞–±–æ—á–∏–µ –≤–µ—Ä—Å–∏–∏ –º–æ–¥–µ–ª–µ–π

---
**–¢–µ–ø–µ—Ä—å —É –≤–∞—Å –µ—Å—Ç—å –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–æ–æ–±—É—á–µ–Ω–∏—è –¥–ª—è RAG Support Bot! üéâ**
